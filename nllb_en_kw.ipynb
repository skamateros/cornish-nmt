{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaDI3MewEZDJ"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcUP06FLFugI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "seed = 19\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auwozjSfXuQP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get rid of duplicates in the source language\n",
        "d = {}\n",
        "with open('/content/drive/MyDrive/data/en-kw.tsv', 'rt') as f:\n",
        "  for line in f:\n",
        "    try:\n",
        "      en, kw = line.strip().split('\\t')\n",
        "      if en not in d:\n",
        "        d[en] = kw\n",
        "    except:\n",
        "      print(line)\n",
        "      continue\n",
        "\n",
        "with open('/content/drive/MyDrive/data/en-kw-no-dups.tsv', 'wt') as f:\n",
        "  for en, kw in d.items():\n",
        "    f.write(f'{en}\\t{kw}\\n')\n"
      ],
      "metadata": {
        "id": "ALRpq8Q2EbL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Om-cH1BzGACK"
      },
      "outputs": [],
      "source": [
        "trans_df = pd.read_csv('/content/drive/MyDrive/data/en-kw-no-dups.tsv', sep=\"\\t\")\n",
        "trans_df = trans_df.dropna()\n",
        "train = trans_df.sample(frac=0.9, random_state=seed)\n",
        "test = trans_df.drop(train.index)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_df = pd.read_csv('/content/drive/MyDrive/data/cornwall-council-2025-09-17.csv', sep=\",\", header= None, names = [\"en\", \"kw\"])\n",
        "trans_df = trans_df.dropna()\n",
        "train = trans_df.sample(frac=0.95, random_state=seed)\n",
        "test = trans_df.drop(train.index)"
      ],
      "metadata": {
        "id": "v6oy8ouWoqfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "id": "QHFDIL3io8X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using tatoeba for train and wikimedia for test\n",
        "train = pd.read_csv('/content/drive/MyDrive/data/tatoeba.tsv', sep=\"\\t\")\n",
        "test = pd.read_csv('/content/drive/MyDrive/data/wikimedia.tsv', sep=\"\\t\")\n",
        "train = train.dropna()\n",
        "test = test.dropna()\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "id": "5nCNk4vUO2Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps2tb8lbLTKe"
      },
      "outputs": [],
      "source": [
        "from transformers import NllbTokenizer\n",
        "\n",
        "def fix_tokenizer(tokenizer, new_lang=\"cor_Latn\"):\n",
        "\n",
        "    # Add as a new special token in the tokenizer\n",
        "    if new_lang not in tokenizer.get_vocab():\n",
        "        tokenizer.add_special_tokens({\"additional_special_tokens\": [new_lang]})\n",
        "        print(f\"Added new language token: {new_lang}\")\n",
        "    else:\n",
        "        print(f\"Language token {new_lang} already exists in vocab.\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3bq8a66Sf90"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "transfer_embeds = True\n",
        "\n",
        "# loading the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "# patching them\n",
        "fix_tokenizer(tokenizer)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# fixing the new/moved token embeddings in the model\n",
        "added_token_id = tokenizer.convert_tokens_to_ids('cor_Latn')\n",
        "similar_lang_id = tokenizer.convert_tokens_to_ids('cym_Latn')\n",
        "embeds = model.model.shared.weight.data\n",
        "\n",
        "# initializing new language token with welsh embeddings\n",
        "if transfer_embeds:\n",
        "  embeds[added_token_id] = embeds[similar_lang_id]\n",
        "\n",
        "# initialising randomly\n",
        "else:\n",
        "  embedding_dim = embeds.shape[1]\n",
        "  embeds[added_token_id] = torch.randn(embedding_dim) * model.config.init_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4uDiZugVlpb"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm, trange\n",
        "from transformers.optimization import Adafactor\n",
        "from transformers import get_constant_schedule_with_warmup\n",
        "\n",
        "def cleanup():\n",
        "    # Free up GPU memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrJR3PzUVoFS"
      },
      "outputs": [],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SOi1CSVVsHn"
      },
      "outputs": [],
      "source": [
        "optimizer = Adafactor(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    scale_parameter=False,\n",
        "    relative_step=False,\n",
        "    lr=1e-4,\n",
        "    clip_threshold=1.0,\n",
        "    weight_decay=1e-3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnzwxDXuVxP-"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "max_length = 128\n",
        "warmup_steps = 500\n",
        "training_steps = len(train) // batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITlR2qiCWATk"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7huv7Fm9WCSm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_batch_pairs(batch_size, data=train):\n",
        "    xx, yy = [], []\n",
        "    for _ in range(batch_size):\n",
        "        item = data.iloc[random.randint(0, len(data)-1)]\n",
        "        xx.append(item['en'])\n",
        "        yy.append(item['kw'])\n",
        "    return xx, yy\n",
        "\n",
        "print(get_batch_pairs(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUcd1uLNjwAK"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "x, y, loss = None, None, None\n",
        "cleanup()\n",
        "\n",
        "lang1 = 'eng_Latn'\n",
        "lang2 = 'cor_Latn'\n",
        "\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    tq = trange(training_steps, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for i in tq:\n",
        "      xx, yy = get_batch_pairs(batch_size)\n",
        "      try:\n",
        "          tokenizer.src_lang = lang1\n",
        "          x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "          tokenizer.src_lang = lang2\n",
        "          y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
        "          y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
        "\n",
        "          loss = model(**x, labels=y.input_ids).loss\n",
        "          loss.backward()\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad(set_to_none=True)\n",
        "          scheduler.step()\n",
        "\n",
        "      except RuntimeError as e:\n",
        "          optimizer.zero_grad(set_to_none=True)\n",
        "          x, y, loss = None, None, None\n",
        "          cleanup()\n",
        "          print('error', max(len(s) for s in xx + yy), e)\n",
        "          continue\n",
        "\n",
        "      if i % 1000 == 0:\n",
        "          print(i, np.mean(losses[-1000:]))\n",
        "\n",
        "# model.save_pretrained(MODEL_SAVE_PATH)\n",
        "# tokenizer.save_pretrained(MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDzFjxc6p2l5"
      },
      "outputs": [],
      "source": [
        "def translate(text, src_lang='eng_Latn', tgt_lang='cor_Latn', a=16, b=1.5, max_input_length=1024, **kwargs):\n",
        "    tokenizer.src_lang = src_lang\n",
        "    tokenizer.tgt_lang = tgt_lang\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=max_input_length)\n",
        "    result = model.generate(\n",
        "        **inputs.to(model.device),\n",
        "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
        "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    return tokenizer.batch_decode(result, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OulChu5HqQDk"
      },
      "outputs": [],
      "source": [
        "lang1 = 'eng_Latn'\n",
        "lang2 = 'cor_Latn'\n",
        "\n",
        "xx, yy = get_batch_pairs(1, data=test)\n",
        "print(xx)\n",
        "print(yy)\n",
        "model.eval()\n",
        "print(translate(xx[0], lang1, lang2, no_repeat_ngram_size=3, num_beams=5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TWieTCKxCC2"
      },
      "outputs": [],
      "source": [
        "# SAVE MODEL\n",
        "MODEL_SAVE_PATH = '/content/drive/MyDrive/models/nllb-eng-cor-v7'\n",
        "model.save_pretrained(MODEL_SAVE_PATH)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqq59q5gW7W_"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "# LOAD MODEL\n",
        "model_load_name = '/content/drive/MyDrive/models/nllb-eng-cor-v7'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_load_name).cuda()\n",
        "tokenizer = NllbTokenizer.from_pretrained(model_load_name)\n",
        "fix_tokenizer(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwDvG_vPYEKz"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "test['translated'] = [translate(t, 'eng_Latn', 'cor_Latn', no_repeat_ngram_size=3, num_beams=5)[0] for t in tqdm(test.en)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igmyO-2b2ZR6"
      },
      "outputs": [],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKTem60KYJIv"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "bleu_calc = sacrebleu.BLEU()\n",
        "chrf_calc = sacrebleu.CHRF(word_order=2)  # ChrF++"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIwIusLZJgu"
      },
      "outputs": [],
      "source": [
        "print(bleu_calc.corpus_score(test['translated'].tolist(), [test['kw'].tolist()]))\n",
        "print(chrf_calc.corpus_score(test['translated'].tolist(), [test['kw'].tolist()]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test.to_csv('/content/drive/MyDrive/data/translations.tsv', sep = \"\\t\")"
      ],
      "metadata": {
        "id": "AmOCK29RVG2r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}